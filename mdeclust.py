# Parallel and non-parallel (standard) implementation of
# Memetic Differential Evolution (MDEClust) algorithm [1]
# for the Euclidean Minimum Sum-of-Squares Clustering (MSSC) problem
# Programmed by Rustam Mussabayev (rmusab@gmail.com)
# 20 August 2022

# Original article with algorithm description:
# [1] Pierluigi Mansueto, Fabio Schoen. Memetic differential evolution methods for clustering problems. Pattern Recognition, Volume 114, 2021, 107849
# https://doi.org/10.1016/j.patcog.2021.107849

import time
import numpy as np
from numba import njit, prange, objmode
from munkres import munkres
from hamerly import hamerly_kmeans


# Squared Euclidean distances
@njit
def distance_mat(X,Y):
    out = np.dot(X, Y.T)
    NX = np.sum(X*X, axis=1)
    NY = np.sum(Y*Y, axis=1)
    for i in range(X.shape[0]):
        for j in range(Y.shape[0]):
            out[i,j] = NX[i] - 2.*out[i,j] + NY[j]
    return out


@njit
def rep_nan(D): #D[np.isnan(D)] = np.nanmax(D)+1.0
    val = np.nanmax(D)+1.0
    for i in range(D.shape[0]):
        for j in range(D.shape[1]):
            if np.isnan(D[i,j]):
                D[i,j] = val

                
# Return True if two solutions A and B (assignment vectors) are equal.
@njit
def check_identity(A,B,k):
    m = A.shape[0]    
    assert B.shape[0] == m
    mapping = np.full(k, -1)
    identical = True
    for i in range(m):
        if mapping[A[i]] == -1:
            mapping[A[i]] = B[i]
        else:
            if mapping[A[i]] != B[i]:
                identical = False
                break
    return identical


# Total difference among function values of all members in the population
# The clustering algorithm is stopped when there is no significant difference among function 
# values of all members in the population.
@njit
def diversity(objectives):
    n = objectives.shape[0]
    val = 0.0
    for i in range(n):
        for j in range(i+1,n):
            val += abs(objectives[i]-objectives[j])
    return val


# The greedy matching algorithm makes the locally optimal choice at each stage 
# with the intent of finding a global optimum: given two solutions A and B, it finds 
# for each center of A the nearest center of B which has not been assigned/matched yet.
@njit
def greedy_matching(A,B):
    m = A.shape[0]
    D = distance_mat(A,B)
    rep_nan(D)    
    matched = np.full(m, False)
    matching = np.full(m, -1)
    for i in range(m):
        D[i][matched] = np.inf
        matching[i] = np.argmin(D[i])
        matched[matching[i]] = True
    return matching
    

# In order to prevent the population collapsing to a single solution, a mutation operator 
# is frequently used to diversify the population. The introduction of this random modification 
# is done through a biased relocation of a center of the offspring O generated by the crossover operator.
# This operation leads to some changes in the center coordinate matrix of O and, subsequently, 
# to an offspring characterized by new features which have not been inherited from the parents.        
@njit
def mutate(centers, center_ind, points, alpha):
    m, n = points.shape
    k = centers.shape[0]
    target_mask = np.sum(np.isnan(centers), axis = 1) == 0
    target_mask[center_ind] = False
    n_target_clusters = np.sum(target_mask)
    new_ind = -1
    if n_target_clusters > 0:
        d = np.full(m, np.inf)
        target_centers = centers[target_mask]
        # Each point points[i], is re-assigned to the closest possible center among the kâˆ’1 available ones.
        for i in range(m):
            for j in range(n_target_clusters):
                dist = 0.0
                for h in range(n):
                    dist += (target_centers[j,h] - points[i,h])**2
                d[i] = min(np.sqrt(dist),d[i])
        # For each ith point, calculate the probability P[i] to be selected as the new cluster center.
        pd = np.full(m, 0.0)
        for i in range(m):
            for j in range(n):
                pd[i] += (points[i,j] - d[i])**2
            pd[i] = np.sqrt(pd[i])
        sum_pd = np.sum(pd)        
        current_pot = 0.0
        P = np.full(m, 0.0)
        for i in range(m):
            P[i] = ((1-alpha)*1/m)+(alpha*pd[i]/sum_pd)
            current_pot += P[i]
        # The choice is done by roulette wheel, which is a randomized operation applied in order to 
        # select potentially useful point.
        rand_val = np.random.random_sample() * current_pot
        cum_sum = 0.0
        for i in range(m):
            cum_sum += P[i]
            if cum_sum >= rand_val:
                new_ind = i
                break
    else:
        new_ind = np.random.randint(m)
    if new_ind > -1:
        centers[center_ind,:] = points[new_ind,:]

        
@njit(parallel = False)
def mdeclust_nonparallel(points, k = 3, population_size=150, tol=0.0001, nmax=5000, matching_mode=0, mutation=False, alpha=0.5, n_attempts = 3, printing=False):
    """
    Standard (non-parallel) implementation of Memetic Differential Evolution (MDEClust) algorithm [1] 
    for the Euclidean Minimum Sum-of-Squares Clustering (MSSC) problem. 
    Programmed by Rustam Mussabayev (rmusab@gmail.com), 20 August 2022.

    Original article with algorithm description:
    [1] Pierluigi Mansueto, Fabio Schoen. Memetic differential evolution methods for clustering problems.
    Pattern Recognition, Volume 114, 2021, 107849. https://doi.org/10.1016/j.patcog.2021.107849

    @param points: Training instances to cluster as matrix of shape (n_samples, n_features).
    @param k: The number of clusters to form as well as the number of centers to generate.
    @param population_size: The number of complete clustering solutions to form the population.
    @param tol: The threshold of the population diversity measure (used as one of the stopping criteria).
    @param nmax: The number of consecutive iterations have been performed without any improvement in the
    best solution to stop the algorithm.
    @param matching_mode: 0 = exact matching (Hungarian algorithm); 1 = greedy matching.
    @param mutation: If True, then the mutation operator will be applied for each offspring.
    @param alpha: The value of alpha influences the selection in the mutation roulette wheel, ranging from
    alpha = 0 (uniform probability) to alpha = 1 (greedy choice).
    @param n_attempts: Represents the number of selection attempts made to choose three distinct solutions
    as parents for producing an offspring.
    @param printing: If True, then the intermediate attributes of the clustering process are printed.

    @attribute objective: Sum of squared distances of points to their closest cluster center.
    @attribute centers: Coordinates of cluster centers.
    @attribute assignment: Labels of each point.
    @attribute n_distances: Number of calculated distances.
    @attribute n_executions: Number of executed runs of the K-means algorithm.
    @attribute best_time: Time spent before obtaining the best solution.
    @attribute best_n_execs: Number of K-means executions before obtaining the best solution. 
    @attribute best_n_dists: Number of distance calculations before obtaining the best solution.    
    """    
    
    with objmode(start_time = 'float64'):
        start_time = time.perf_counter()    
    
    m, n = points.shape
    assert (k > 0) and (population_size > 4) and (tol > 0.0) and (nmax > 0) 
    assert (matching_mode in range(2)) and (0.0 <= alpha <= 1.0) and (n_attempts > 0)
        
    objectives = np.full(population_size, 0.0)
    centers = np.full((population_size, k, n), np.nan)
    assignment = np.full((population_size, m), -1)    
    n_dists = 0
    n_execs = 0
    n_idle = 0
    best_time = np.inf
    best_n_execs = 0
    best_n_dists = 0
    best_objective = np.inf
        
    # The initial population is created through population_size independent runs of the Forgy K-MEANS algorithm.
    for i in range(population_size):
        center_inds = np.random.choice(m, k, replace=False)
        centers[i,:,:] = points[center_inds,:]
        objectives[i], _, assignment[i], numDistances = hamerly_kmeans(points, centers[i])
        n_dists += numDistances
        n_execs += 1
        if objectives[i] < best_objective:        
            with objmode(current_time = 'float64'):
                current_time = time.perf_counter() - start_time
            best_time = current_time
            best_n_execs = n_execs
            best_n_dists = n_dists
            best_objective = objectives[i]
        
    if printing:
        print('objective', 'n_executions', 'diversity')
        print(best_objective, best_n_execs, diversity(objectives))
        
    while (n_idle < nmax) and (diversity(objectives) > tol): # while stopping criterion is not satisfied do
       
        for i in range(population_size): # Iterate over all solutions in the population
            
            if (n_idle < nmax) and (diversity(objectives) > tol):
                # Randomly select S1, S2, S3 all different from each other and from Si
                parents = np.full(3,-1)
                n_att = 0
                identical = True
                while identical:
                    parents = np.random.choice(np.array([j for j in range(population_size) if j != i]), 3, replace=False)
                    n_att += 1
                    if n_att > n_attempts:
                        break                        
                    basis = np.full(4, i)
                    basis[:3] = parents[:]
                    identical = False
                    for a in range(4):
                        for b in range(a+1,4):
                            if check_identity(assignment[basis[a]],assignment[basis[b]],k):
                                identical = True
                                break
                        if identical:
                            break
                            
                S = np.copy(centers[np.argmin(objectives)])
                S1 = np.copy(centers[parents[0]])
                S2 = np.copy(centers[parents[1]])
                S3 = np.copy(centers[parents[2]])
                # Each time an operation between two solutions must be performed, 
                # we require that the centers in both solutions are matched according to their similarity.
                if matching_mode == 0: # exact matching using Kuhn-Munkres (Hungarian) algorithm
                    # S3 is taken as the reference solution whose centers are matched to the ones of S1 and S2
                    D1 = distance_mat(S3,S1)
                    n_dists += k*k
                    rep_nan(D1)                             
                    inds1 = munkres(D1)
                    
                    D2 = distance_mat(S3,S2)
                    n_dists += k*k                    
                    rep_nan(D2)                                
                    inds2 = munkres(D2)
                    
                    inds3 = np.arange(k)                    
                else: # greedy_matching
                    # Two centers in two different solutions are matched to each other if and only if 
                    # both have been matched to the same center of the best solution                    
                    inds1 = greedy_matching(S,S1)
                    inds2 = greedy_matching(S,S2)
                    inds3 = greedy_matching(S,S3)
                    n_dists += 3*k*k
                S1 = S1[inds1]
                S2 = S2[inds2]
                S3 = S3[inds3]                
                F = np.random.uniform(0.5, 0.8) # F is chosen randomly in the interval [0.5, 0.8] at each crossover operator execution
                # Crossover is composed by a linear combination of the three selected solutions:
                O = S1+F*(S2-S3) # Apply the Differential Evolution (DE) algorithm to calculate the new offspring solution O                    
                if mutation:
                    # A random center is selected with uniform probability and mutated.
                    mutate(O, np.random.randint(k), points, alpha)
                    n_dists += m*(k-1)+m

                # Repair solution having degenerate clusters
                degenerate_mask = np.sum(np.isnan(O), axis = 1) > 0
                n_degenerates = np.sum(degenerate_mask)
                if n_degenerates > 0:
                    degenerate_inds = np.arange(k)[degenerate_mask]
                    for center_ind in degenerate_inds:
                        mutate(O, center_ind, points, alpha)
                        n_dists += m*(k-1)+m

                # Apply local search (K-MEANS) to offspring O to obtain a solution O';
                new_objective, _, new_assignment, numDistances = hamerly_kmeans(points, O)
                n_dists += numDistances
                n_execs += 1
                
                # The returned solution O' is compared with the considered solution S_i in terms of 
                # objective function value: if solution O' is better than S_i, this latter one is 
                # replaced in the population; otherwise, the solution O' is discarded.
                if new_objective < objectives[i]:
                   
                    if new_objective < np.min(objectives): # Check if the solution O' has the best objective
                        n_idle = 0
                        with objmode(time_now = 'float64'):
                            time_now = time.perf_counter() - start_time
                        best_time = time_now
                        best_n_execs = n_execs
                        best_n_dists = n_dists
                        if printing:
                            print(new_objective, best_n_execs, diversity(objectives))
                    objectives[i] = new_objective
                    centers[i] = np.copy(O)
                    assignment[i] = np.copy(new_assignment)
                else:
                    n_idle += 1

    if printing:
        print('')
        print('n_executions = ',n_execs)
        print('n_idle = ',n_idle)
        print('diversity = ',diversity(objectives))
        print('n_distances = ',n_dists)
        print('')        
    
    best = np.argmin(objectives)

    return objectives[best], centers[best], assignment[best], n_dists, n_execs, best_time, best_n_execs, best_n_dists




@njit(parallel = True)
def mdeclust_parallel(points, k = 3, population_size=150, tol=0.0001, nmax=5000, matching_mode=0, mutation=False, alpha=0.5, n_attempts = 3, printing=False):
    """
    Parallel implementation of Memetic Differential Evolution (MDEClust) algorithm [1] for the Euclidean Minimum
    Sum-of-Squares Clustering (MSSC) problem. Programmed by Rustam Mussabayev (rmusab@gmail.com),
    20 August 2022.

    Original article with algorithm description:
    [1] Pierluigi Mansueto, Fabio Schoen. Memetic differential evolution methods for clustering problems.
    Pattern Recognition, Volume 114, 2021, 107849. https://doi.org/10.1016/j.patcog.2021.107849

    @param points: Training instances to cluster as matrix of shape (n_samples, n_features).
    @param k: The number of clusters to form as well as the number of centers to generate.
    @param population_size: The number of complete clustering solutions to form the population.
    @param tol: The threshold of the population diversity measure (used as one of the stopping criteria).
    @param nmax: The number of consecutive iterations have been performed without any improvement in the
    best solution to stop the algorithm.
    @param matching_mode: 0 = exact matching (Hungarian algorithm); 1 = greedy matching.
    @param mutation: If True, then the mutation operator will be applied for each offspring.
    @param alpha: The value of alpha influences the selection in the mutation roulette wheel, ranging from
    alpha = 0 (uniform probability) to alpha = 1 (greedy choice).
    @param n_attempts: Represents the number of selection attempts made to choose three distinct solutions
    as parents for producing an offspring.
    @param printing: If True, then the intermediate attributes of the clustering process are printed.

    @attribute objective: Sum of squared distances of points to their closest cluster center.
    @attribute centers: Coordinates of cluster centers.
    @attribute assignment: Labels of each point.
    @attribute n_distances: Number of calculated distances.
    @attribute n_executions: Number of executed runs of the K-means algorithm.
    @attribute best_time: Time spent before obtaining the best solution.
    @attribute best_n_execs: Number of K-means executions before obtaining the best solution. 
    @attribute best_n_dists: Number of distance calculations before obtaining the best solution.    
    """    
    
    with objmode(start_time = 'float64'):
        start_time = time.perf_counter()    
    
    m, n = points.shape
    assert (k > 0) and (population_size > 4) and (tol > 0.0) and (nmax > 0) 
    assert (matching_mode in range(2)) and (0.0 <= alpha <= 1.0) and (n_attempts > 0)
        
    objectives = np.full(population_size, 0.0)
    centers = np.full((population_size, k, n), np.nan)
    assignment = np.full((population_size, m), -1)    
    n_dists = np.full(population_size, 0)
    n_execs = np.full(population_size, 0)
    n_idle = np.full(population_size, 0)
    best_time = np.full(population_size, np.inf)
    best_n_execs = np.full(population_size, 0)
    best_n_dists = np.full(population_size, 0)
        
    # The initial population is created through population_size independent runs of the Forgy K-MEANS algorithm.
    for i in prange(population_size):
        center_inds = np.random.choice(m, k, replace=False)
        centers[i,:,:] = points[center_inds,:]
        objectives[i], _, assignment[i], n_dists[i] = hamerly_kmeans(points, centers[i])
        n_execs[i] += 1
        with objmode(current_time = 'float64'):
            current_time = time.perf_counter() - start_time
        best_time[i] = current_time
        best_n_execs[i] = np.sum(n_execs)
        best_n_dists[i] = np.sum(n_dists)
        
    if printing:
        print('objective', 'n_executions', 'diversity')
        best = np.where(best_time == np.min(best_time[objectives == np.min(objectives)]))[0][0]
        print(objectives[best], best_n_execs[best], diversity(objectives))
        
    while (np.sum(n_idle) < nmax) and (diversity(objectives) > tol): # while stopping criterion is not satisfied do
       
        for i in prange(population_size): # Iterate over all solutions in the population
            
            if (np.sum(n_idle) < nmax) and (diversity(objectives) > tol):
                # Randomly select S1, S2, S3 all different from each other and from Si
                parents = np.full(3,-1)
                n_att = 0
                identical = True
                while identical:
                    parents = np.random.choice(np.array([j for j in range(population_size) if j != i]), 3, replace=False)
                    n_att += 1
                    if n_att > n_attempts:
                        break                        
                    basis = np.full(4, i)
                    basis[:3] = parents[:]
                    identical = False
                    for a in range(4):
                        for b in range(a+1,4):
                            if check_identity(assignment[basis[a]],assignment[basis[b]],k):
                                identical = True
                                break
                        if identical:
                            break
                            
                S = np.copy(centers[np.argmin(objectives)])
                S1 = np.copy(centers[parents[0]])
                S2 = np.copy(centers[parents[1]])
                S3 = np.copy(centers[parents[2]])
                # Each time an operation between two solutions must be performed, 
                # we require that the centers in both solutions are matched according to their similarity.
                if matching_mode == 0: # exact matching using Kuhn-Munkres (Hungarian) algorithm
                    # S3 is taken as the reference solution whose centers are matched to the ones of S1 and S2
                    D1 = distance_mat(S3,S1)
                    n_dists[i] += k*k
                    rep_nan(D1)                             
                    inds1 = munkres(D1)
                    
                    D2 = distance_mat(S3,S2)
                    n_dists[i] += k*k                    
                    rep_nan(D2)                                
                    inds2 = munkres(D2)
                    
                    inds3 = np.arange(k)                    
                else: # greedy_matching
                    # Two centers in two different solutions are matched to each other if and only if 
                    # both have been matched to the same center of the best solution                    
                    inds1 = greedy_matching(S,S1)
                    inds2 = greedy_matching(S,S2)
                    inds3 = greedy_matching(S,S3)
                    n_dists[i] += 3*k*k                    
                S1 = S1[inds1]
                S2 = S2[inds2]
                S3 = S3[inds3]                
                F = np.random.uniform(0.5, 0.8) # F is chosen randomly in the interval [0.5, 0.8] at each crossover operator execution
                # Crossover is composed by a linear combination of the three selected solutions:
                O = S1+F*(S2-S3) # Apply the Differential Evolution (DE) algorithm to calculate the new offspring solution O                    
                if mutation:
                    # A random center is selected with uniform probability and mutated.
                    mutate(O, np.random.randint(k), points, alpha)
                    n_dists[i] += m*(k-1)+m

                # Repair solution having degenerate clusters
                degenerate_mask = np.sum(np.isnan(O), axis = 1) > 0
                n_degenerates = np.sum(degenerate_mask)
                if n_degenerates > 0:
                    degenerate_inds = np.arange(k)[degenerate_mask]
                    for center_ind in degenerate_inds:
                        mutate(O, center_ind, points, alpha)
                        n_dists[i] += m*(k-1)+m

                # Apply local search (K-MEANS) to offspring O to obtain a solution O';
                new_objective, _, new_assignment, numDistances = hamerly_kmeans(points, O)
                n_dists[i] += numDistances
                n_execs[i] += 1
                
                # The returned solution O' is compared with the considered solution S_i in terms of 
                # objective function value: if solution O' is better than S_i, this latter one is 
                # replaced in the population; otherwise, the solution O' is discarded.
                if new_objective < objectives[i]:
                    best_n_execs[i] = np.sum(n_execs)
                    if new_objective < np.min(objectives): # Check if the solution O' has the best objective
                        n_idle[:] = 0
                        if printing:
                            print (new_objective, best_n_execs[i], diversity(objectives))
                    objectives[i] = new_objective
                    centers[i] = np.copy(O)
                    assignment[i] = np.copy(new_assignment)
                    
                    with objmode(time_now = 'float64'):
                        time_now = time.perf_counter() - start_time
                    best_time[i] = time_now                    
                    best_n_dists[i] = np.sum(n_dists)
                else:
                    n_idle[i] += 1                

    n_distances = np.sum(n_dists)
    n_executions = np.sum(n_execs)
    if printing:
        print('')
        print('n_executions = ',n_executions)
        print('n_idle = ',np.sum(n_idle))
        print('diversity = ',diversity(objectives))
        print('n_distances = ',n_distances)
        print('')        
    
   
    best = np.where(best_time == np.min(best_time[objectives == np.min(objectives)]))[0][0]
   
    return objectives[best], centers[best], assignment[best], n_distances, n_executions, best_time[best], best_n_execs[best], best_n_dists[best]

